{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "  You are helpful AI talking to a human\n",
      "\n",
      "  \n",
      "  Human:My name is Heon\n",
      "  You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Heon! How can I assist you today?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=llm,\n",
    "  max_token_limit=120,\n",
    "  memory_key=\"chat_history\"   # 템플릿안에 메모리가 history를 저장하도록 한 곳을 적어준다.\n",
    ")\n",
    "\n",
    "template=\"\"\"\n",
    "\n",
    "  You are helpful AI talking to a human\n",
    "\n",
    "  {chat_history}\n",
    "  Human:{question}\n",
    "  You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "  llm=llm,\n",
    "  memory=memory,\n",
    "  prompt=PromptTemplate.from_template(template),\n",
    "  verbose=True  #chain  프롬프트 로그를 확인 할 수 있다\n",
    "  return_messages=True  #문자열로 바꾸지 말고 실제 메세지로 바꿔달라는 것을 의미\n",
    ")\n",
    "\n",
    "\n",
    "chain.predict(question=\"My name is Heon\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "  You are helpful AI talking to a human\n",
      "\n",
      "  Human: My name is Heon\n",
      "AI: Hello Heon! How can I assist you today?\n",
      "  Human:I live in Seoul\n",
      "  You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great to hear! How can I assist you with information or tasks related to Seoul?\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"I live in Seoul\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\n",
      "  You are helpful AI talking to a human\n",
      "\n",
      "  Human: My name is Heon\n",
      "AI: Hello Heon! How can I assist you today?\n",
      "Human: I live in Seoul\n",
      "AI: That's great to hear! How can I assist you with information or tasks related to Seoul?\n",
      "  Human:What is my name?\n",
      "  You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Heon.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: Heon introduces themselves as living in Seoul. The AI responds by acknowledging Seoul as a vibrant city with a rich history and culture, asking Heon what they enjoy most about living there.\\nHuman: What is my name?\\nAI: I'm sorry, I do not have access to personal information such as your name.\"}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, got set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 15\u001b[0m\n\u001b[1;32m      6\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      8\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationSummaryBufferMemory(\n\u001b[1;32m      9\u001b[0m   llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     10\u001b[0m   max_token_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m,\n\u001b[1;32m     11\u001b[0m   memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m ,  \u001b[38;5;66;03m# 템플릿안에 메모리가 history를 저장하도록 한 곳을 적어준다.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m   return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m#문자열로 바꾸지 말고 실제 메세지로 바꿔달라는 것을 의미\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mChatPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m  \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful AI talking to a human\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m  \u001b[49m\u001b[43mMessagesPlaceholder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#누가 보냈는지 알 수 없는, 예측하기 어려운 메세지의 양과 제한 없는 양의 메세지를 가질 수 있다. \u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m  \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m     23\u001b[0m   llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     24\u001b[0m   memory\u001b[38;5;241m=\u001b[39mmemory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m   \n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     31\u001b[0m chain\u001b[38;5;241m.\u001b[39mpredict(question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy name is Heon\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt/env/lib/python3.11/site-packages/langchain/prompts/chat.py:541\u001b[0m, in \u001b[0;36mChatPromptTemplate.from_messages\u001b[0;34m(cls, messages)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_messages\u001b[39m(\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    505\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[1;32m    506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatPromptTemplate:\n\u001b[1;32m    507\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m        a chat prompt template\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 541\u001b[0m     _messages \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m_convert_to_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[1;32m    544\u001b[0m     input_vars: Set[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt/env/lib/python3.11/site-packages/langchain/prompts/chat.py:541\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_messages\u001b[39m(\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    505\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[1;32m    506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatPromptTemplate:\n\u001b[1;32m    507\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m        a chat prompt template\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 541\u001b[0m     _messages \u001b[38;5;241m=\u001b[39m [\u001b[43m_convert_to_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[1;32m    544\u001b[0m     input_vars: Set[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt/env/lib/python3.11/site-packages/langchain/prompts/chat.py:739\u001b[0m, in \u001b[0;36m_convert_to_message\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    737\u001b[0m message_type_str, template \u001b[38;5;241m=\u001b[39m message\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message_type_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 739\u001b[0m     _message \u001b[38;5;241m=\u001b[39m \u001b[43m_create_template_from_message_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_type_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    741\u001b[0m     _message \u001b[38;5;241m=\u001b[39m message_type_str(prompt\u001b[38;5;241m=\u001b[39mPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(template))\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt/env/lib/python3.11/site-packages/langchain/prompts/chat.py:692\u001b[0m, in \u001b[0;36m_create_template_from_message_type\u001b[0;34m(message_type, template)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a message prompt template from a message type and template string.\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \n\u001b[1;32m    684\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;124;03m    a message prompt template of the appropriate type.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     message: BaseMessagePromptTemplate \u001b[38;5;241m=\u001b[39m \u001b[43mHumanMessagePromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemplate\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m message_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    696\u001b[0m     message \u001b[38;5;241m=\u001b[39m AIMessagePromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(template)\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt/env/lib/python3.11/site-packages/langchain/prompts/chat.py:152\u001b[0m, in \u001b[0;36mBaseStringMessagePromptTemplate.from_template\u001b[0;34m(cls, template, template_format, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_template\u001b[39m(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mcls\u001b[39m: Type[MessagePromptTemplateT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    141\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MessagePromptTemplateT:\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a class from a string template.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m        A new instance of this class.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[43mPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(prompt\u001b[38;5;241m=\u001b[39mprompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt/env/lib/python3.11/site-packages/langchain/prompts/prompt.py:232\u001b[0m, in \u001b[0;36mPromptTemplate.from_template\u001b[0;34m(cls, template, template_format, partial_variables, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_template\u001b[39m(\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    203\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptTemplate:\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a prompt template from a template.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    *Security warning*: Prefer using `template_format=\"f-string\"` instead of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m        The prompt template loaded from the template.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m \u001b[43mget_template_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     _partial_variables \u001b[38;5;241m=\u001b[39m partial_variables \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _partial_variables:\n",
      "File \u001b[0;32m~/Documents/coder/Python/fullstack-gpt/env/lib/python3.11/site-packages/langchain/prompts/base.py:144\u001b[0m, in \u001b[0;36mget_template_variables\u001b[0;34m(template, template_format)\u001b[0m\n\u001b[1;32m    141\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m _get_jinja2_variables_from_template(template)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m template_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    143\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 144\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m _, v, _, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mFormatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     }\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported template format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/string.py:288\u001b[0m, in \u001b[0;36mFormatter.parse\u001b[0;34m(self, format_string)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string):\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _string\u001b[38;5;241m.\u001b[39mformatter_parser(format_string)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, got set"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "  llm=llm,\n",
    "  max_token_limit=120,\n",
    "  memory_key=\"chat_history\" ,  # 템플릿안에 메모리가 history를 저장하도록 한 곳을 적어준다.\n",
    "  return_messages=True  #문자열로 바꾸지 말고 실제 메세지로 바꿔달라는 것을 의미\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", \"You are a helpful AI talking to a human\"),\n",
    "  MessagesPlaceholder(variable_name=\"chat_history\"), #누가 보냈는지 알 수 없는, 예측하기 어려운 메세지의 양과 제한 없는 양의 메세지를 가질 수 있다. \n",
    "  (\"human\",\" {question}\"),\n",
    "])\n",
    "\n",
    "\n",
    "chain = LLMChain(\n",
    "  llm=llm,\n",
    "  memory=memory,\n",
    "  prompt=prompt,\n",
    "  verbose=True, #chain  프롬프트 로그를 확인 할 수 있다\n",
    "  \n",
    ")\n",
    "\n",
    "\n",
    "chain.predict(question=\"My name is Heon\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
